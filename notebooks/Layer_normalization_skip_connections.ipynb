{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[0.2260, 0.3470, 0.000, 0.2216, 0.000, 0.000],\n",
    "                      [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.000]])\n",
    "\n",
    "mean = torch.mean(input,1, keepdim=True)\n",
    "var = torch.var(input, 1, keepdim=True)\n",
    "\n",
    "## layer Normalization\n",
    "layer_norm_inp = (input -mean)/torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6160,  1.4126, -0.8719,  0.5870, -0.8719, -0.8719],\n",
       "        [-0.0187,  0.1121, -1.0877,  1.5173,  0.5646, -1.0877]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean after layer normalization is tensor([[-3.9736e-08],\n",
      "        [-1.9868e-08]])\n",
      "The variance after layer normalization is tensor([[1.0000],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "layer_mean = layer_norm_inp.mean(dim =-1, keepdim=True)\n",
    "layer_var = layer_norm_inp.var(dim =-1, keepdim=True)\n",
    "\n",
    "print(f\"The mean after layer normalization is {layer_mean}\")\n",
    "print(f\"The variance after layer normalization is {layer_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
       "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "import math\n",
    "scale = nn.Parameter(torch.ones(6))\n",
    "shift = nn.Parameter(torch.zeros(6))\n",
    "\n",
    "mean = input.mean(dim =-1, keepdim=True)\n",
    "var = input.var(dim =-1, keepdim= True)\n",
    "\n",
    "linear_norm_input = (input-mean)/torch.sqrt(var+eps)\n",
    "linear_norm_input = linear_norm_input*scale + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6159,  1.4123, -0.8717,  0.5869, -0.8717, -0.8717],\n",
       "        [-0.0187,  0.1121, -1.0875,  1.5171,  0.5646, -1.0875]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_norm_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_normalization(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mean = x.mean(dim =-1, keepdim = True)\n",
    "        var = x.var(dim =-1, keepdim = True, unbiased = False)\n",
    "        layer_norm = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        print(layer_norm)\n",
    "        print(self.scale*layer_norm+ self.shift)\n",
    "\n",
    "        return self.scale*layer_norm + self.shift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6746,  1.5470, -0.9548,  0.6429, -0.9548, -0.9548],\n",
      "        [-0.0205,  0.1228, -1.1913,  1.6619,  0.6184, -1.1913]])\n",
      "tensor([[ 0.6746,  1.5470, -0.9548,  0.6429, -0.9548, -0.9548],\n",
      "        [-0.0205,  0.1228, -1.1913,  1.6619,  0.6184, -1.1913]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = Layer_normalization(emb_dim =6)\n",
    "out_ln = ln(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.9671e-08],\n",
      "        [-1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.1994],\n",
      "        [1.1996]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln_mean = out_ln.mean(dim =-1, keepdim=True)\n",
    "ln_var = out_ln.var(dim =-1, keepdim=True)\n",
    "print(ln_mean)\n",
    "print(ln_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6746,  1.5470, -0.9548,  0.6429, -0.9548, -0.9548],\n",
       "        [-0.0205,  0.1228, -1.1913,  1.6619,  0.6184, -1.1913]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Shirtcut connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "       return 0.5*x*(1+ torch.tanh(torch.sqrt(2/(torch.Tensor([math.pi])))*(x + 0.044715*(torch.pow(x, 3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortconnections(nn.Module):\n",
    "    def __init__(self, layer_size, use_shortcut):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0], layer_size[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[1], layer_size[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[2], layer_size[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[3], layer_size[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[4], layer_size[5]), GELU())]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x+layer_output\n",
    "\n",
    "            else :\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = [3,3,3,3,3,1]\n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model_without_shortcut = Shortconnections(layer_size, use_shortcut= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    loss = nn.MSELoss()\n",
    "    targets = torch.tensor([[0.]])\n",
    "    loss = loss(output, targets)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152039906941354\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0014432291500270367\n",
      "layers.1.0.weight has gradient mean of 0.004846952389925718\n",
      "layers.2.0.weight has gradient mean of 0.004138893447816372\n",
      "layers.3.0.weight has gradient mean of 0.005915115587413311\n",
      "layers.4.0.weight has gradient mean of 0.032659437507390976\n"
     ]
    }
   ],
   "source": [
    "model_without_shortcut = Shortconnections(layer_size, use_shortcut= True)\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
