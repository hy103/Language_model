{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859e7b35-2edd-45c3-b9c5-0665c5b6c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs 25\n",
      "Epoch : 0\n",
      "Epoch 1 (Step 000000):Train loss  10.008111Val loss  10.082557\n",
      "Epoch 1 (Step 000005):Train loss  8.051633Val loss  8.311679\n",
      "Epoch 1 (Step 000010):Train loss  6.861059Val loss  7.301111\n",
      "Epoch 1 (Step 000015):Train loss  6.486801Val loss  6.923510\n",
      "Epoch 1 (Step 000020):Train loss  6.428917Val loss  6.882326\n",
      "Epoch 1 (Step 000025):Train loss  6.365217Val loss  6.879770\n",
      "Epoch 1 (Step 000030):Train loss  5.939267Val loss  6.855207\n",
      "Epoch 1 (Step 000035):Train loss  6.118840Val loss  6.774461\n",
      "Epoch 1 (Step 000040):Train loss  5.959351Val loss  6.751552\n",
      "Epoch 1 (Step 000045):Train loss  6.018787Val loss  6.709007\n",
      "Arguments are extremely vulgar,,�,�,�,,,,�,�,�,�,,�,�,�,�,�,�,,�,,�,�,�,,�,�,�,�\n",
      "Epoch : 1\n",
      "Epoch 2 (Step 000050):Train loss  5.921027Val loss  6.682971\n",
      "Epoch 2 (Step 000055):Train loss  5.933471Val loss  6.639628\n",
      "Epoch 2 (Step 000060):Train loss  5.985428Val loss  6.617052\n",
      "Epoch 2 (Step 000065):Train loss  5.893101Val loss  6.730160\n",
      "Epoch 2 (Step 000070):Train loss  13.515546Val loss  11.664947\n",
      "Epoch 2 (Step 000075):Train loss  5.979082Val loss  6.696936\n",
      "Epoch 2 (Step 000080):Train loss  5.928828Val loss  6.686341\n",
      "Epoch 2 (Step 000085):Train loss  5.986396Val loss  6.718374\n",
      "Epoch 2 (Step 000090):Train loss  5.961574Val loss  6.724355\n",
      "Epoch 2 (Step 000095):Train loss  5.817397Val loss  6.703006\n",
      "Arguments are extremely vulgar,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�\n",
      "Epoch : 2\n",
      "Epoch 3 (Step 000100):Train loss  5.891873Val loss  6.691281\n",
      "Epoch 3 (Step 000105):Train loss  5.891132Val loss  6.697528\n",
      "Epoch 3 (Step 000110):Train loss  5.872970Val loss  6.720554\n",
      "Epoch 3 (Step 000115):Train loss  5.890509Val loss  6.724530\n",
      "Epoch 3 (Step 000120):Train loss  5.926123Val loss  6.715078\n",
      "Epoch 3 (Step 000125):Train loss  5.871926Val loss  6.718166\n",
      "Epoch 3 (Step 000130):Train loss  5.776077Val loss  6.716897\n",
      "Epoch 3 (Step 000135):Train loss  5.793416Val loss  6.709979\n",
      "Epoch 3 (Step 000140):Train loss  5.693527Val loss  6.719534\n",
      "Arguments are extremely vulgar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Epoch : 3\n",
      "Epoch 4 (Step 000145):Train loss  5.870027Val loss  6.727141\n",
      "Epoch 4 (Step 000150):Train loss  5.862412Val loss  6.743840\n",
      "Epoch 4 (Step 000155):Train loss  5.803541Val loss  6.777249\n",
      "Epoch 4 (Step 000160):Train loss  5.876699Val loss  6.767402\n",
      "Epoch 4 (Step 000165):Train loss  5.830420Val loss  6.756671\n",
      "Epoch 4 (Step 000170):Train loss  5.952195Val loss  6.742755\n",
      "Epoch 4 (Step 000175):Train loss  5.880070Val loss  6.765630\n",
      "Epoch 4 (Step 000180):Train loss  5.859346Val loss  6.747366\n",
      "Epoch 4 (Step 000185):Train loss  5.867830Val loss  6.719946\n",
      "Epoch 4 (Step 000190):Train loss  5.902363Val loss  6.700104\n",
      "Arguments are extremely vulgar,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�\n",
      "Epoch : 4\n",
      "Epoch 5 (Step 000195):Train loss  5.823225Val loss  6.691774\n",
      "Epoch 5 (Step 000200):Train loss  5.837255Val loss  6.715784\n",
      "Epoch 5 (Step 000205):Train loss  5.759460Val loss  6.741219\n",
      "Epoch 5 (Step 000210):Train loss  5.760791Val loss  6.757049\n",
      "Epoch 5 (Step 000215):Train loss  5.689842Val loss  6.738531\n",
      "Epoch 5 (Step 000220):Train loss  5.747104Val loss  6.721867\n",
      "Epoch 5 (Step 000225):Train loss  5.834039Val loss  6.692037\n",
      "Epoch 5 (Step 000230):Train loss  5.713923Val loss  6.665737\n",
      "Epoch 5 (Step 000235):Train loss  5.887848Val loss  6.658847\n",
      "Arguments are extremely vulgar,�,�,�, and,�, and, and, and,�, and, and,�, and, and, and, and,�, and, and, and, and, and, and,�,�\n",
      "Epoch : 5\n",
      "Epoch 6 (Step 000240):Train loss  5.735693Val loss  6.661250\n",
      "Epoch 6 (Step 000245):Train loss  5.805295Val loss  6.659293\n",
      "Epoch 6 (Step 000250):Train loss  5.724433Val loss  6.627826\n",
      "Epoch 6 (Step 000255):Train loss  5.617663Val loss  6.591251\n",
      "Epoch 6 (Step 000260):Train loss  5.585771Val loss  6.478432\n",
      "Epoch 6 (Step 000265):Train loss  5.345153Val loss  6.472440\n",
      "Epoch 6 (Step 000270):Train loss  5.350101Val loss  6.417180\n",
      "Epoch 6 (Step 000275):Train loss  5.422646Val loss  6.461291\n",
      "Epoch 6 (Step 000280):Train loss  5.314040Val loss  6.480638\n",
      "Epoch 6 (Step 000285):Train loss  5.446326Val loss  6.444325\n",
      "Arguments are extremely vulgar.                     the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch : 6\n",
      "Epoch 7 (Step 000290):Train loss  5.473356Val loss  6.436876\n",
      "Epoch 7 (Step 000295):Train loss  5.568208Val loss  6.430269\n",
      "Epoch 7 (Step 000300):Train loss  5.299375Val loss  6.361834\n",
      "Epoch 7 (Step 000305):Train loss  5.466631Val loss  6.370446\n",
      "Epoch 7 (Step 000310):Train loss  5.361675Val loss  6.306335\n",
      "Epoch 7 (Step 000315):Train loss  5.136435Val loss  6.330622\n",
      "Epoch 7 (Step 000320):Train loss  5.075052Val loss  6.294142\n",
      "Epoch 7 (Step 000325):Train loss  5.269642Val loss  6.250114\n",
      "Epoch 7 (Step 000330):Train loss  5.048804Val loss  6.321119\n",
      "Epoch 7 (Step 000335):Train loss  5.243245Val loss  6.299902\n",
      "Arguments are extremely vulgar.  �, “I,“I,““I,“I,” said the Miller the Miller, and the Miller the the the Miller the Miller the Miller the the the the Miller the the the Miller\n",
      "Epoch : 7\n",
      "Epoch 8 (Step 000340):Train loss  5.139860Val loss  6.388286\n",
      "Epoch 8 (Step 000345):Train loss  5.521615Val loss  6.431742\n",
      "Epoch 8 (Step 000350):Train loss  5.593876Val loss  6.605869\n",
      "Epoch 8 (Step 000355):Train loss  5.379232Val loss  6.597740\n",
      "Epoch 8 (Step 000360):Train loss  5.376052Val loss  6.494824\n",
      "Epoch 8 (Step 000365):Train loss  5.249280Val loss  6.393027\n",
      "Epoch 8 (Step 000370):Train loss  5.268647Val loss  6.364895\n",
      "Epoch 8 (Step 000375):Train loss  5.093616Val loss  6.249818\n",
      "Epoch 8 (Step 000380):Train loss  4.947104Val loss  6.264624\n",
      "Arguments are extremely vulgar.  ““““““““““““““““” said the he the he the he the he the he the he\n",
      "Epoch : 8\n",
      "Epoch 9 (Step 000385):Train loss  5.011730Val loss  6.261293\n",
      "Epoch 9 (Step 000390):Train loss  5.006081Val loss  6.337541\n",
      "Epoch 9 (Step 000395):Train loss  5.075578Val loss  6.319527\n",
      "Epoch 9 (Step 000400):Train loss  5.017130Val loss  6.275838\n",
      "Epoch 9 (Step 000405):Train loss  4.897889Val loss  6.299273\n",
      "Epoch 9 (Step 000410):Train loss  4.930662Val loss  6.312920\n",
      "Epoch 9 (Step 000415):Train loss  4.994722Val loss  6.267703\n",
      "Epoch 9 (Step 000420):Train loss  4.785648Val loss  6.229030\n",
      "Epoch 9 (Step 000425):Train loss  5.184381Val loss  6.248753\n",
      "Epoch 9 (Step 000430):Train loss  4.693801Val loss  6.228118\n",
      "Arguments are extremely vulgar, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Epoch : 9\n",
      "Epoch 10 (Step 000435):Train loss  4.936939Val loss  6.218744\n",
      "Epoch 10 (Step 000440):Train loss  4.643136Val loss  6.173165\n",
      "Epoch 10 (Step 000445):Train loss  4.894580Val loss  6.207191\n",
      "Epoch 10 (Step 000450):Train loss  4.834766Val loss  6.267500\n",
      "Epoch 10 (Step 000455):Train loss  5.099290Val loss  6.196597\n",
      "Epoch 10 (Step 000460):Train loss  4.714630Val loss  6.148043\n",
      "Epoch 10 (Step 000465):Train loss  4.749627Val loss  6.179417\n",
      "Epoch 10 (Step 000470):Train loss  4.528457Val loss  6.181882\n",
      "Epoch 10 (Step 000475):Train loss  4.527978Val loss  6.190272\n",
      "Arguments are extremely vulgar.  �, and he had the Miller, and he had he had, and the Miller the Miller.  “I am, and he cried the Miller, and the Miller, and the Miller, and the Miller the Rocket, and the\n",
      "Epoch : 10\n",
      "Epoch 11 (Step 000480):Train loss  4.437563Val loss  6.173148\n",
      "Epoch 11 (Step 000485):Train loss  4.515496Val loss  6.212926\n",
      "Epoch 11 (Step 000490):Train loss  4.462687Val loss  6.226798\n",
      "Epoch 11 (Step 000495):Train loss  4.489893Val loss  6.278270\n",
      "Epoch 11 (Step 000500):Train loss  4.414749Val loss  6.273135\n",
      "Epoch 11 (Step 000505):Train loss  4.489579Val loss  6.182776\n",
      "Epoch 11 (Step 000510):Train loss  4.416748Val loss  6.240065\n",
      "Epoch 11 (Step 000515):Train loss  4.461551Val loss  6.154433\n",
      "Epoch 11 (Step 000520):Train loss  4.424854Val loss  6.131780\n",
      "Epoch 11 (Step 000525):Train loss  4.369928Val loss  6.105694\n",
      "Arguments are extremely vulgar, and the the the the, and the the the the the the the the the the the, and the the the the the the the the the the the the the the the the the the the the the, and the, and the the the\n",
      "Epoch : 11\n",
      "Epoch 12 (Step 000530):Train loss  4.151375Val loss  6.091540\n",
      "Epoch 12 (Step 000535):Train loss  4.176847Val loss  6.114764\n",
      "Epoch 12 (Step 000540):Train loss  4.239827Val loss  6.162927\n",
      "Epoch 12 (Step 000545):Train loss  4.299987Val loss  6.117986\n",
      "Epoch 12 (Step 000550):Train loss  4.150622Val loss  6.096234\n",
      "Epoch 12 (Step 000555):Train loss  4.348805Val loss  6.155690\n",
      "Epoch 12 (Step 000560):Train loss  4.202390Val loss  6.105103\n",
      "Epoch 12 (Step 000565):Train loss  4.048721Val loss  6.106761\n",
      "Epoch 12 (Step 000570):Train loss  4.015070Val loss  6.132713\n",
      "Epoch 12 (Step 000575):Train loss  4.237764Val loss  6.172192\n",
      "Arguments are extremely vulgar, and the the.  He was the Miller the children, and the same the children the same, and the same the children and the children, and the Miller the children and the children and the children, and the garden, and the thorn,\n",
      "Epoch : 12\n",
      "Epoch 13 (Step 000580):Train loss  4.053068Val loss  6.049089\n",
      "Epoch 13 (Step 000585):Train loss  4.124718Val loss  6.073869\n",
      "Epoch 13 (Step 000590):Train loss  4.143021Val loss  6.028858\n",
      "Epoch 13 (Step 000595):Train loss  4.054311Val loss  6.052656\n",
      "Epoch 13 (Step 000600):Train loss  3.931820Val loss  6.034064\n",
      "Epoch 13 (Step 000605):Train loss  4.015284Val loss  6.260624\n",
      "Epoch 13 (Step 000610):Train loss  4.018666Val loss  6.187223\n",
      "Epoch 13 (Step 000615):Train loss  3.996320Val loss  6.163894\n",
      "Epoch 13 (Step 000620):Train loss  4.003035Val loss  6.153566\n",
      "Arguments are extremely vulgar, and the the the. “” said the Miller, “” said the Miller he cried the Miller” said the Miller he cried the Miller he cried the Miller he cried the Miller he cried the Miller he cried the\n",
      "Epoch : 13\n",
      "Epoch 14 (Step 000625):Train loss  3.813353Val loss  6.060052\n",
      "Epoch 14 (Step 000630):Train loss  3.875676Val loss  6.137715\n",
      "Epoch 14 (Step 000635):Train loss  3.739603Val loss  6.154493\n",
      "Epoch 14 (Step 000640):Train loss  3.820098Val loss  6.045717\n",
      "Epoch 14 (Step 000645):Train loss  3.629411Val loss  5.978913\n",
      "Epoch 14 (Step 000650):Train loss  3.689389Val loss  6.090907\n",
      "Epoch 14 (Step 000655):Train loss  3.696023Val loss  6.088397\n",
      "Epoch 14 (Step 000660):Train loss  3.662206Val loss  6.098344\n",
      "Epoch 14 (Step 000665):Train loss  3.723189Val loss  6.025829\n",
      "Epoch 14 (Step 000670):Train loss  3.654009Val loss  6.077937\n",
      "Arguments are extremely vulgar, and the the the. “” “I have the Rocket, and the Rocket, and the King.  “I have the Prince, and the Prince.  “I have no doubt.  The Prince\n",
      "Epoch : 14\n",
      "Epoch 15 (Step 000675):Train loss  3.582874Val loss  6.119549\n",
      "Epoch 15 (Step 000680):Train loss  3.675387Val loss  5.998337\n",
      "Epoch 15 (Step 000685):Train loss  3.475771Val loss  5.979777\n",
      "Epoch 15 (Step 000690):Train loss  3.659362Val loss  6.042388\n",
      "Epoch 15 (Step 000695):Train loss  3.506033Val loss  6.097052\n",
      "Epoch 15 (Step 000700):Train loss  3.329144Val loss  6.023139\n",
      "Epoch 15 (Step 000705):Train loss  3.408461Val loss  6.070768\n",
      "Epoch 15 (Step 000710):Train loss  3.376786Val loss  6.096931\n",
      "Epoch 15 (Step 000715):Train loss  3.423156Val loss  5.989806\n",
      "Arguments are extremely vulgar.  He had the Miller, and the Giant.  He was the Giant, and the Giant the Giant the Giant.  He was the Giant, and the Giant, and the Giant, and the Giant, and the Giant.  He was\n",
      "Epoch : 15\n",
      "Epoch 16 (Step 000720):Train loss  3.133017Val loss  5.987479\n",
      "Epoch 16 (Step 000725):Train loss  3.217170Val loss  5.971890\n",
      "Epoch 16 (Step 000730):Train loss  3.048215Val loss  6.047398\n",
      "Epoch 16 (Step 000735):Train loss  3.193401Val loss  6.048486\n",
      "Epoch 16 (Step 000740):Train loss  3.122660Val loss  6.091923\n",
      "Epoch 16 (Step 000745):Train loss  2.908533Val loss  6.075998\n",
      "Epoch 16 (Step 000750):Train loss  3.046552Val loss  6.080652\n",
      "Epoch 16 (Step 000755):Train loss  2.880655Val loss  6.063470\n",
      "Epoch 16 (Step 000760):Train loss  2.919206Val loss  5.990033\n",
      "Epoch 16 (Step 000765):Train loss  2.776293Val loss  6.067567\n",
      "Arguments are extremely vulgar, and the Miller the Miller, and the Miller, and the terr, and the butterflies, and the great the great fruit. “” said the Rocket, “” said the Rocket, “” “\n",
      "Epoch : 16\n",
      "Epoch 17 (Step 000770):Train loss  2.799645Val loss  6.031205\n",
      "Epoch 17 (Step 000775):Train loss  2.860215Val loss  6.003435\n",
      "Epoch 17 (Step 000780):Train loss  2.676596Val loss  6.118610\n",
      "Epoch 17 (Step 000785):Train loss  2.594739Val loss  6.037650\n",
      "Epoch 17 (Step 000790):Train loss  2.450391Val loss  6.093341\n",
      "Epoch 17 (Step 000795):Train loss  2.545007Val loss  6.104070\n",
      "Epoch 17 (Step 000800):Train loss  2.487758Val loss  6.083930\n",
      "Epoch 17 (Step 000805):Train loss  2.384658Val loss  5.993682\n",
      "Epoch 17 (Step 000810):Train loss  2.416119Val loss  6.060828\n",
      "Epoch 17 (Step 000815):Train loss  2.474811Val loss  6.019301\n",
      "Arguments are extremely vulgar. “” answered the Miller, “I am going to the Rocket.  I am going to be a great favourite.  In fact, and the Prince.  In fact, and the Court Gazette.  I will be\n",
      "Epoch : 17\n",
      "Epoch 18 (Step 000820):Train loss  2.478511Val loss  6.123388\n",
      "Epoch 18 (Step 000825):Train loss  2.306129Val loss  6.133059\n",
      "Epoch 18 (Step 000830):Train loss  2.275502Val loss  6.169804\n",
      "Epoch 18 (Step 000835):Train loss  2.337401Val loss  6.212179\n",
      "Epoch 18 (Step 000840):Train loss  2.398721Val loss  6.278603\n",
      "Epoch 18 (Step 000845):Train loss  2.374355Val loss  6.129700\n",
      "Epoch 18 (Step 000850):Train loss  2.355025Val loss  6.152961\n",
      "Epoch 18 (Step 000855):Train loss  2.263426Val loss  6.106503\n",
      "Epoch 18 (Step 000860):Train loss  2.212314Val loss  6.089987\n",
      "Arguments are extremely vulgar, and the greathips will, and the autumn.  The birds the children they will sing, and the Snow danced the essence the trees, and the trees, and the trees.  In the trees, and the trees were the trees were so\n",
      "Epoch : 18\n",
      "Epoch 19 (Step 000865):Train loss  2.193500Val loss  6.350527\n",
      "Epoch 19 (Step 000870):Train loss  1.995069Val loss  6.242276\n",
      "Epoch 19 (Step 000875):Train loss  1.898353Val loss  6.141054\n",
      "Epoch 19 (Step 000880):Train loss  1.993045Val loss  6.237686\n",
      "Epoch 19 (Step 000885):Train loss  1.989344Val loss  6.172834\n",
      "Epoch 19 (Step 000890):Train loss  1.803666Val loss  6.212037\n",
      "Epoch 19 (Step 000895):Train loss  1.792110Val loss  6.192884\n",
      "Epoch 19 (Step 000900):Train loss  1.773138Val loss  6.212009\n",
      "Epoch 19 (Step 000905):Train loss  1.768356Val loss  6.205613\n",
      "Epoch 19 (Step 000910):Train loss  1.730972Val loss  6.092826\n",
      "Arguments are extremely vulgar.  The birds was so long I have no doubt that I have like to the Frost.  The poor Snow were so beautiful.  The poor house the North Wind the trees were so beautiful sapphires.  The boy were so that\n",
      "Epoch : 19\n",
      "Epoch 20 (Step 000915):Train loss  1.562921Val loss  6.247027\n",
      "Epoch 20 (Step 000920):Train loss  1.635766Val loss  6.344744\n",
      "Epoch 20 (Step 000925):Train loss  1.627374Val loss  6.528367\n",
      "Epoch 20 (Step 000930):Train loss  1.493589Val loss  6.293593\n",
      "Epoch 20 (Step 000935):Train loss  1.354270Val loss  6.318150\n",
      "Epoch 20 (Step 000940):Train loss  1.476207Val loss  6.298701\n",
      "Epoch 20 (Step 000945):Train loss  1.409663Val loss  6.384203\n",
      "Epoch 20 (Step 000950):Train loss  1.461789Val loss  6.252474\n",
      "Epoch 20 (Step 000955):Train loss  1.397657Val loss  6.239572\n",
      "Arguments are extremely vulgar, and the Prince.  The birds will give it, and the North Wind, and the Giant that the Snow in her throat. “She is more,” said the Rocket, “This is not be not be drowned.\n",
      "Epoch : 20\n",
      "Epoch 21 (Step 000960):Train loss  1.301950Val loss  6.336802\n",
      "Epoch 21 (Step 000965):Train loss  1.313604Val loss  6.357635\n",
      "Epoch 21 (Step 000970):Train loss  1.375092Val loss  6.504578\n",
      "Epoch 21 (Step 000975):Train loss  1.236664Val loss  6.324367\n",
      "Epoch 21 (Step 000980):Train loss  1.057505Val loss  6.438218\n",
      "Epoch 21 (Step 000985):Train loss  1.126311Val loss  6.371707\n",
      "Epoch 21 (Step 000990):Train loss  1.229195Val loss  6.438435\n",
      "Epoch 21 (Step 000995):Train loss  0.995173Val loss  6.426266\n",
      "Epoch 21 (Step 001000):Train loss  0.979204Val loss  6.481917\n",
      "Epoch 21 (Step 001005):Train loss  1.050317Val loss  6.395816\n",
      "Arguments are extremely vulgar,” said the Tree, “you must build it out of music by moonlight, and stain it to be let off.  Really, and the children is a red rose, and the thorn.  She is a red rose like\n",
      "Epoch : 21\n",
      "Epoch 22 (Step 001010):Train loss  0.993037Val loss  6.424156\n",
      "Epoch 22 (Step 001015):Train loss  0.905281Val loss  6.555334\n",
      "Epoch 22 (Step 001020):Train loss  0.917834Val loss  6.650670\n",
      "Epoch 22 (Step 001025):Train loss  0.848215Val loss  6.540889\n",
      "Epoch 22 (Step 001030):Train loss  0.844627Val loss  6.635081\n",
      "Epoch 22 (Step 001035):Train loss  0.797331Val loss  6.586395\n",
      "Epoch 22 (Step 001040):Train loss  0.660263Val loss  6.624774\n",
      "Epoch 22 (Step 001045):Train loss  0.807271Val loss  6.646334\n",
      "Epoch 22 (Step 001050):Train loss  0.739151Val loss  6.584396\n",
      "Epoch 22 (Step 001055):Train loss  0.723479Val loss  6.684967\n",
      "Arguments are extremely vulgar, and the great morning, and the garden. “What a silly question the Nightingale,” cried the Rocket. “I am going to say to you to you to the King.  You must not to them\n",
      "Epoch : 22\n",
      "Epoch 23 (Step 001060):Train loss  0.814269Val loss  6.590947\n",
      "Epoch 23 (Step 001065):Train loss  0.648454Val loss  6.712258\n",
      "Epoch 23 (Step 001070):Train loss  0.628074Val loss  6.753506\n",
      "Epoch 23 (Step 001075):Train loss  0.560104Val loss  6.822465\n",
      "Epoch 23 (Step 001080):Train loss  0.713128Val loss  6.767165\n",
      "Epoch 23 (Step 001085):Train loss  0.676752Val loss  6.815341\n",
      "Epoch 23 (Step 001090):Train loss  0.646115Val loss  6.831263\n",
      "Epoch 23 (Step 001095):Train loss  0.571908Val loss  6.718762\n",
      "Epoch 23 (Step 001100):Train loss  0.530448Val loss  6.746660\n",
      "Arguments are extremely vulgar, and the Prince. “My friends,” said the Rocket, “you have given you, and be able to be happy, and make the deal box in strange as the King.  They must be happy, and the\n",
      "Epoch : 23\n",
      "Epoch 24 (Step 001105):Train loss  0.473097Val loss  6.756463\n",
      "Epoch 24 (Step 001110):Train loss  0.499831Val loss  6.755021\n",
      "Epoch 24 (Step 001115):Train loss  0.504331Val loss  6.882648\n",
      "Epoch 24 (Step 001120):Train loss  0.437414Val loss  6.846297\n",
      "Epoch 24 (Step 001125):Train loss  0.469276Val loss  6.826011\n",
      "Epoch 24 (Step 001130):Train loss  0.463664Val loss  6.745978\n",
      "Epoch 24 (Step 001135):Train loss  0.385886Val loss  6.899502\n",
      "Epoch 24 (Step 001140):Train loss  0.409579Val loss  6.787706\n",
      "Epoch 24 (Step 001145):Train loss  0.417527Val loss  7.017678\n",
      "Epoch 24 (Step 001150):Train loss  0.350863Val loss  6.916754\n",
      "Arguments are extremely vulgar.  The birds that it, and the North Wind, and the Hail, and the Frost, and the Snow danced.  Then the King’s shoulder, and the trees, and the Giant to the Giant.  It was lying in\n",
      "Epoch : 24\n",
      "Epoch 25 (Step 001155):Train loss  0.370559Val loss  7.056138\n",
      "Epoch 25 (Step 001160):Train loss  0.395505Val loss  7.003198\n",
      "Epoch 25 (Step 001165):Train loss  0.363064Val loss  7.110957\n",
      "Epoch 25 (Step 001170):Train loss  0.352392Val loss  6.957354\n",
      "Epoch 25 (Step 001175):Train loss  0.296919Val loss  7.030567\n",
      "Epoch 25 (Step 001180):Train loss  0.313260Val loss  7.086405\n",
      "Epoch 25 (Step 001185):Train loss  0.312154Val loss  7.090866\n",
      "Epoch 25 (Step 001190):Train loss  0.298789Val loss  6.970945\n",
      "Epoch 25 (Step 001195):Train loss  0.288420Val loss  6.984404\n",
      "Arguments are extremely vulgar, I shall hold her in my arms, and she will be able, and my shoulder, and her hand will be clasped in mine.  But there is no red rose in my garden, so I shall sit lonely, and my heart will\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from GPT_Model import GPTModel\n",
    "from create_dataloader import create_dataloader_v1\n",
    "from text_token_text import text_to_token_ids, token_ids_to_text, generate_text_simple\n",
    "\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader)==0:\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "\n",
    "    else: \n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i< num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            total_loss+= loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches= eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches= eval_iter)\n",
    "\n",
    "        model.train()\n",
    "        return train_loss , val_loss\n",
    "    \n",
    "def generate_and_print(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids =generate_text_simple(model = model, idx = encoded,\n",
    "                                        max_new_tokens = 50, context_size = context_size)\n",
    "        \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, \n",
    "                       optimizer, device, num_epochs, \n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses , val_losses ,track_tokens_seen = [], [], []\n",
    "\n",
    "    tokens_seen , global_step = 0, -1\n",
    "    print(f\"Number of epochs {num_epochs}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch : {epoch}\")\n",
    "        model.train()\n",
    "        for input_batch, target_batch in (train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step+=1\n",
    "\n",
    "            if global_step % eval_freq ==0:\n",
    "                train_loss , val_loss  = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Epoch {epoch+1} (Step {global_step:06d}):\"\n",
    "                      f\"Train loss {train_loss : 3f}\"\n",
    "                      f\"Val loss {val_loss : 3f}\")\n",
    "                \n",
    "\n",
    "        generate_and_print(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 256,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_layers\" : 12,\n",
    "    \"n_heads\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}\n",
    "\n",
    "file_path = \"verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio*len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "train_loader = create_dataloader_v1(train_data,\n",
    "                                    batch_size =2, \n",
    "                                     max_length =  GPT_CONFIG_124M[\"context_length\"],\n",
    "                                     stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "                                     drop_last = True,\n",
    "                                     shuffle = True,\n",
    "                                     num_workers =0)\n",
    "\n",
    "\n",
    "val_loader = create_dataloader_v1(val_data,\n",
    "                                    batch_size =2, \n",
    "                                     max_length =  GPT_CONFIG_124M[\"context_length\"],\n",
    "                                     stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "                                     drop_last = True,\n",
    "                                     shuffle = True,\n",
    "                                     num_workers =0)\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "num_epochs = 25\n",
    "train_losses , val_losses, tokens_seen = train_model_simple(model,\n",
    "                                                            train_loader, val_loader, optimizer,\n",
    "                                                            device, num_epochs=num_epochs, eval_freq =5, eval_iter = 5,\n",
    "                                                            start_context= \"Arguments are extremely vulgar\", tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def main():\n",
    "    pass   \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cc48a-55c9-4f46-834a-56118f87833c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293f78f-3c71-4fae-9fee-fc39657b4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arguments are extremely vulgar, for everybody in good society holds exactly the same opinions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
